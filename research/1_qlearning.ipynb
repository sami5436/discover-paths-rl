{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef55a659",
   "metadata": {},
   "source": [
    "--- 1. GLOBAL VARIABLES & CONFIGURATION ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f5bd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2253614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_WIDTH = 5\n",
    "GRID_HEIGHT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d56e4c",
   "metadata": {},
   "source": [
    "Using coordinates as (x, y) which map to (col, row)\n",
    "\n",
    "(0, 0) is top-left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c60542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_PICKUP_LOCS = {(1, 2): 5, (3, 3): 5}  # (x, y): block_count\n",
    "INITIAL_DROPOFF_LOCS = {(0, 0): 0, (0, 4): 0, (2, 2): 0, (4, 3): 0} # (x, y): block_count\n",
    "TOTAL_BLOCKS_AT_START = sum(INITIAL_PICKUP_LOCS.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83eab9",
   "metadata": {},
   "source": [
    "--- Agent Start Positions ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bdec57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_F_START = {'x': 0, 'y': 2, 'has_block': False}\n",
    "AGENT_M_START = {'x': 4, 'y': 2, 'has_block': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b9ceab",
   "metadata": {},
   "source": [
    "--- RL Parameters ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8de06766",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.3  # Alpha (α)\n",
    "DISCOUNT_FACTOR = 0.5  # Gamma (γ)\n",
    "ACTIONS = ['North', 'South', 'East', 'West', 'Pickup', 'Dropoff']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d906a00",
   "metadata": {},
   "source": [
    "--- Global State Variables (will be modified during the simulation) ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c431ca3",
   "metadata": {},
   "source": [
    "Agent states (dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0063cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_f = {}\n",
    "agent_m = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5fe793",
   "metadata": {},
   "source": [
    "World state (dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c0810205",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_locs = {}\n",
    "dropoff_locs = {}\n",
    "total_blocks_delivered = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecdcaf",
   "metadata": {},
   "source": [
    "The Q-Tables. We'll use Option (a) from the prompt: one Q-table per agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ccea5",
   "metadata": {},
   "source": [
    "Key: state tuple. Value: dictionary of {action: q-value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e597374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_f = {}\n",
    "q_table_m = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf008386",
   "metadata": {},
   "source": [
    "--- 2. ENVIRONMENT & AGENT FUNCTIONS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0903b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_world_and_agents(): # general function for both sarsa and q-learning\n",
    "    \"\"\"Resets the environment and agents to their initial states.\"\"\"\n",
    "\n",
    "    global pickup_locs, dropoff_locs, total_blocks_delivered, agent_f, agent_m # sets some variables as global to modify them\n",
    "    \n",
    "    # Reset world\n",
    "    pickup_locs = copy.deepcopy(INITIAL_PICKUP_LOCS) # uses deepcopy to avoid reference issues and \n",
    "    dropoff_locs = copy.deepcopy(INITIAL_DROPOFF_LOCS)\n",
    "    total_blocks_delivered = 0 # resets the delivered blocks count\n",
    "\n",
    "    # Reset agents\n",
    "    agent_f = copy.deepcopy(AGENT_F_START) # resets agent F to its initial state\n",
    "    agent_m = copy.deepcopy(AGENT_M_START) # resets agent M to its initial state\n",
    "\n",
    "    print(\"World and agents reset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "61c4ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal_state(): # general function for both sarsa and q-learning\n",
    "    \"\"\"Checks if all blocks have been delivered.\"\"\"\n",
    "    return total_blocks_delivered == TOTAL_BLOCKS_AT_START # this checks if this state is terminal (terminal being the end state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ecccbf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_actions(agent, other_agent): # general function for both sarsa and q-learning\n",
    "    \"\"\"Returns a list of all valid actions for the agent.\"\"\"\n",
    "    \n",
    "    possible = [] # list of possible actions (since some actions may be invalid such as moving out of bounds or into another agent)\n",
    "   \n",
    "    agent_pos = (agent['x'], agent['y']) # current position of the agent\n",
    "    other_pos = (other_agent['x'], other_agent['y']) # current position of the other agent\n",
    "    \n",
    "    # Check movement actions\n",
    "    for action, (dx, dy) in {'North': (0, -1), 'South': (0, 1), 'East': (1, 0), 'West': (-1, 0)}.items(): # for each movement action and its corresponding delta x and delta y\n",
    "        next_x, next_y = agent['x'] + dx, agent['y'] + dy # calculate the next position after the move\n",
    "        \n",
    "        # Check boundaries\n",
    "        if 0 <= next_x < GRID_WIDTH and 0 <= next_y < GRID_HEIGHT: # check if the next position is within grid bounds\n",
    "            # Check collision with other agent\n",
    "            if (next_x, next_y) != other_pos: # check if the next position does not collide with the other agent\n",
    "                possible.append(action) # add the action to possible actions list\n",
    "\n",
    "    # Check Pickup\n",
    "    if not agent['has_block'] and agent_pos in pickup_locs and pickup_locs[agent_pos] > 0: # if the agent does not have a block and is at a pickup location with available blocks\n",
    "        possible.append('Pickup')\n",
    "\n",
    "    # Check Dropoff\n",
    "    if agent['has_block'] and agent_pos in dropoff_locs: # if the agent has a block and is at a dropoff location\n",
    "        # Assuming dropoff locs have infinite capacity for this simple version\n",
    "        # The prompt implies a capacity (e.g., 5), which you can add here\n",
    "        possible.append('Dropoff')\n",
    "        \n",
    "    return possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "69d0c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(agent, other_agent, action): # general function for both sarsa and q-learning\n",
    "    \"\"\"\n",
    "    Applies an agent's action to the world.\n",
    "    action is one of ['North', 'South', 'East', 'West', 'Pickup', 'Dropoff'].\n",
    "    MODIFIES the global 'agent', 'pickup_locs', 'dropoff_locs', 'total_blocks_delivered'.\n",
    "    Returns (reward, new_agent_state_tuple)\n",
    "    \"\"\"\n",
    "    global total_blocks_delivered # We need to modify this global \n",
    "    agent_pos = (agent['x'], agent['y']) # get current position of the agent\n",
    "    other_pos = (other_agent['x'], other_agent['y']) # get current position of the other agent\n",
    "\n",
    "    reward = -1  # Default cost for a movement step\n",
    "    is_valid = True\n",
    "\n",
    "    new_x, new_y = agent['x'], agent['y'] # Initialize new position variables\n",
    "\n",
    "    if action == 'North': # if the following action is taken\n",
    "        new_y -= 1\n",
    "    elif action == 'South':\n",
    "        new_y += 1\n",
    "    elif action == 'East':\n",
    "        new_x += 1\n",
    "    elif action == 'West':\n",
    "        new_x -= 1\n",
    "    elif action == 'Pickup': # however, if the action is pickup, then \n",
    "        # check if pickup is valid\n",
    "        if not agent['has_block'] and agent_pos in pickup_locs and pickup_locs[agent_pos] > 0:\n",
    "            agent['has_block'] = True # agent now has a block\n",
    "            pickup_locs[agent_pos] -= 1 # decrease the number of blocks at that pickup location\n",
    "            reward = 13 # reward for successful pickup. this number can be adjusted\n",
    "        else:\n",
    "            is_valid = False # invalid action\n",
    "            reward = -10  # Penalize invalid action (trying to pick up when already carrying a block)\n",
    "    elif action == 'Dropoff': # if the action is dropoff\n",
    "        # check if dropoff is valid\n",
    "        if agent['has_block'] and agent_pos in dropoff_locs:\n",
    "            agent['has_block'] = False # agent no longer has a block\n",
    "            dropoff_locs[agent_pos] += 1 # increase the number of blocks at that dropoff location\n",
    "            total_blocks_delivered += 1 # increment the total blocks delivered\n",
    "            reward = 13 # reward for successful dropoff. this number can be adjusted\n",
    "        else:\n",
    "            is_valid = False # invalid action\n",
    "            reward = -10 # Penalize invalid action (trying to drop off without a block or at an invalid location)\n",
    "\n",
    "    # Check for boundary violations (for move actions)\n",
    "    if action in ['North', 'South', 'East', 'West']: # if the action is a movement action\n",
    "        # check for out of bounds\n",
    "        if not (0 <= new_x < GRID_WIDTH and 0 <= new_y < GRID_HEIGHT):\n",
    "            is_valid = False\n",
    "            reward = -10 # penalize for moving out of bounds\n",
    "            new_x, new_y = agent['x'], agent['y']  # stay in place\n",
    "        \n",
    "        # Check for blockage (collision)\n",
    "        elif (new_x, new_y) == other_pos:\n",
    "            is_valid = False\n",
    "            reward = -10 # penalize for collision with other agent\n",
    "            new_x, new_y = agent['x'], agent['y']  # Stay in place\n",
    "\n",
    "    # If a move was valid, update agent's position\n",
    "    if action in ['North', 'South', 'East', 'West'] and is_valid:\n",
    "        agent['x'], agent['y'] = new_x, new_y\n",
    "\n",
    "    # Return the reward and the *new* state of the agent who just moved\n",
    "    new_state = get_state(agent, other_agent) \n",
    "    return reward, new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2960e4",
   "metadata": {},
   "source": [
    "--- 3. REINFORCEMENT LEARNING FUNCTIONS ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7b5bd7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(agent, other_agent): # general function for both sarsa and q-learning\n",
    "    \"\"\"\n",
    "    Generates the state tuple. This is the key to the Q-table.\n",
    "    State = (agent_x, agent_y, agent_has_block, other_agent_x, other_agent_y)\n",
    "    This implements option (a) from the prompt [cite: 173-176]\n",
    "    \"\"\"\n",
    "    # returns the positions and block status of both agents as a tuple\n",
    "    return (agent['x'], agent['y'], agent['has_block'], other_agent['x'], other_agent['y']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "34f6f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_value(q_table, state, action): # general function for both sarsa and q-learning \n",
    "    \"\"\"Helper to get Q-value for a state-action pair, initializing if not present. this gets the q value from the q-table\"\"\"\n",
    "    \n",
    "    if state not in q_table: # if the state is not in the q-table\n",
    "        # Initialize all actions with 0 for this new state\n",
    "        q_table[state] = {act: 0.0 for act in ACTIONS} # each action needs to have a position + initial Q-value of 0.0\n",
    "        \n",
    "    # This handles cases where the action might be invalid (e.g., 'Pickup' at a D loc)\n",
    "    # We still want to be able to look it up without an error\n",
    "    if action not in q_table[state]:\n",
    "        q_table[state][action] = 0.0\n",
    "            \n",
    "    return q_table[state][action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "38431ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_q_action(q_table, state, possible_actions): # general function for both sarsa and q-learning\n",
    "    \"\"\"Finds the action with the highest Q-value from the possible actions.\"\"\"\n",
    "    if not possible_actions:\n",
    "        return None, 0.0  # No action possible (pickup/dropoff not available. obviously should not happen in practice)\n",
    "    \n",
    "    max_q = -float('inf')\n",
    "    best_actions = []\n",
    "\n",
    "    for action in possible_actions: # for each possible action (north, south, east, west, pickup, dropoff)\n",
    "        q_val = get_q_value(q_table, state, action) # get the q value for that state and action\n",
    "        if q_val > max_q: # if the q value is greater than the current max q value\n",
    "            max_q = q_val # update max q value\n",
    "            best_actions = [action] # start a new list of best actions\n",
    "        elif q_val == max_q:\n",
    "            best_actions.append(action)\n",
    "\n",
    "    # \"break ties by rolling a dice\"\n",
    "    return random.choice(best_actions), max_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "220270cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(q_table, state, possible_actions, policy): # general function for both sarsa and q-learning\n",
    "    \"\"\"Chooses an action based on the current policy. Current policy being one of 'PRANDOM', 'PEXPLOIT', 'PGREEDY'.\"\"\"\n",
    "    \n",
    "    if not possible_actions: # if there are no possible actions\n",
    "        return None  # Agent is trapped\n",
    "    \n",
    "    \n",
    "    # \"If pickup and dropoff is applicable, choose this operator\" \n",
    "    # This rule overrides all policies\n",
    "    if 'Pickup' in possible_actions:\n",
    "        return 'Pickup'\n",
    "    if 'Dropoff' in possible_actions:\n",
    "        return 'Dropoff'\n",
    "\n",
    "    # --- PRANDOM --- \n",
    "    if policy == 'PRANDOM':\n",
    "        return random.choice(possible_actions) # choose a random action from possible actions\n",
    "\n",
    "    # --- PEXPLOIT --- \n",
    "    elif policy == 'PEXPLOIT': \n",
    "        if random.random() < 0.8:  # 80% chance to exploit\n",
    "            # we call the general function to get the best action for that state + all possible actions\n",
    "            best_action, _ = get_max_q_action(q_table, state, possible_actions)\n",
    "            return best_action \n",
    "        else:  # 20% chance to explore - we go against the best action\n",
    "            # we still need to get the best action to avoid it\n",
    "            best_action, _ = get_max_q_action(q_table, state, possible_actions) \n",
    "            \n",
    "            # \"choose a different applicable operator randomly\"\n",
    "            # we make sure to not choose the best action so we choose from the other actions\n",
    "            exploration_choices = [a for a in possible_actions if a != best_action]\n",
    "            if not exploration_choices:\n",
    "                return best_action  # No other choice, so just return the best one\n",
    "            return random.choice(exploration_choices)\n",
    "\n",
    "    # --- PGREEDY --- \n",
    "    elif policy == 'PGREEDY': \n",
    "        # we call the general function to get the best action for that state + all possible actions\n",
    "        best_action, _ = get_max_q_action(q_table, state, possible_actions)\n",
    "        return best_action\n",
    "\n",
    "    raise ValueError(f\"Unknown policy: {policy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b2330ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(q_table, old_state, action, reward, new_state, new_possible_actions): # this function is only for q-learning\n",
    "    \"\"\"Performs the Q-Learning update rule.\"\"\"\n",
    "    # Get the Q-value for the action we took\n",
    "    # this works by calling the general function to get the q value for that state and action\n",
    "    old_q = get_q_value(q_table, old_state, action) \n",
    "\n",
    "    # get the max Q-value for the next state\n",
    "    # using the new state and possible actions, we call the general function to get the max q action\n",
    "    _ , max_next_q = get_max_q_action(q_table, new_state, new_possible_actions)\n",
    "\n",
    "    # q-learning formula: Q(s,a) = Q(s,a) + α * [R + γ * max_a' Q(s',a') - Q(s,a)]\n",
    "    temporal_difference = reward + (DISCOUNT_FACTOR * max_next_q) - old_q\n",
    "    new_q = old_q + (LEARNING_RATE * temporal_difference)\n",
    "\n",
    "    q_table[old_state][action] = new_q # make sure to update the q-table specific to the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "25188777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sarsa_table(q_table, old_state, action, reward, new_state, next_action): # this function is only for sarsa\n",
    "    \"\"\"Performs the SARSA update rule.\"\"\"\n",
    "    old_q = get_q_value(q_table, old_state, action) # get the q value for the old state and action\n",
    "    next_q = get_q_value(q_table, new_state, next_action) # get the q value for the next state and next action\n",
    "    # sarsa formula Q(s,a) = Q(s,a) + α * [R + γ * Q(s',a') - Q(s,a)]\n",
    "    temporal_difference = reward + (DISCOUNT_FACTOR * next_q) - old_q \n",
    "    new_q = old_q + (LEARNING_RATE * temporal_difference)\n",
    "    q_table[old_state][action] = new_q # make sure to update the q-table specific to the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c521d",
   "metadata": {},
   "source": [
    "--- 4. MAIN SIMULATION LOOP (Example: Experiment 1.c) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "43e00e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    \"\"\"Main experiment loop.\"\"\"\n",
    "    # --- Experiment Setup ---\n",
    "    # Let's set up Experiment 1.c: 500 steps PRANDOM, 7500 steps PEXPLOIT\n",
    "    TOTAL_STEPS = 8000\n",
    "    POLICY_SCHEDULE = [(500, 'PRANDOM'), (7500, 'PEXPLOIT')]\n",
    "    CURRENT_ALGORITHM = 'Q_LEARNING' # vs 'SARSA'\n",
    "\n",
    "    # --- Statistics Tracking ---\n",
    "    total_reward_f = 0\n",
    "    total_reward_m = 0\n",
    "    terminal_states_reached = 0\n",
    "    steps_per_run = [] # List to store how many steps it took to reach each terminal state\n",
    "    current_run_steps = 0\n",
    "\n",
    "    # --- Initialization ---\n",
    "    reset_world_and_agents()\n",
    "\n",
    "    # The simulation alternates turns: F, M, F, M, ...\n",
    "    # \"female agent acting first\" [cite: 169]\n",
    "    agents = [agent_f, agent_m]\n",
    "    q_tables = [q_table_f, q_table_m]\n",
    "    agent_names = ['F', 'M']\n",
    "\n",
    "    current_policy = POLICY_SCHEDULE[0][1]\n",
    "    policy_switch_step = POLICY_SCHEDULE[0][0]\n",
    "    policy_index = 0\n",
    "\n",
    "    print(f\"Starting simulation: {TOTAL_STEPS} steps.\")\n",
    "    print(f\"Initial policy: {current_policy}\")\n",
    "\n",
    "    for step in range(TOTAL_STEPS):\n",
    "        \n",
    "        # --- Policy Switching Logic ---\n",
    "        if policy_index < len(POLICY_SCHEDULE) and step >= policy_switch_step:\n",
    "            policy_index += 1\n",
    "            if policy_index < len(POLICY_SCHEDULE):\n",
    "                switch_step, new_policy = POLICY_SCHEDULE[policy_index]\n",
    "                current_policy = new_policy\n",
    "                policy_switch_step += switch_step # Next switch step is cumulative\n",
    "                print(f\"--- Step {step}: Switching policy to {current_policy} ---\")\n",
    "            \n",
    "        # --- Agent Turn ---\n",
    "        agent_turn = step % 2  # 0 for F, 1 for M\n",
    "        \n",
    "        acting_agent = agents[agent_turn]\n",
    "        other_agent = agents[1 - agent_turn]\n",
    "        acting_q_table = q_tables[agent_turn]\n",
    "        \n",
    "        # --- 1. Get State & Possible Actions ---\n",
    "        old_state = get_state(acting_agent, other_agent)\n",
    "        possible_actions = get_possible_actions(acting_agent, other_agent)\n",
    "        \n",
    "        # --- 2. Choose Action ---\n",
    "        action = choose_action(acting_q_table, old_state, possible_actions, current_policy)\n",
    "        \n",
    "        if action is None:\n",
    "            # Agent is trapped, no action to take. Skip turn.\n",
    "            continue\n",
    "            \n",
    "        # --- 3. Apply Action & Get Reward ---\n",
    "        # This function modifies the agent's state *in-place*\n",
    "        reward, new_state = apply_action(acting_agent, other_agent, action)\n",
    "        \n",
    "        # --- 4. Get New State's Possible Actions (for Q-Learning) ---\n",
    "        # Note: The 'other_agent' for this *new* state is the same 'other_agent'\n",
    "        # because it hasn't moved yet.\n",
    "        new_possible_actions = get_possible_actions(acting_agent, other_agent)\n",
    "\n",
    "        # --- 5. Update Q-Table ---\n",
    "        if CURRENT_ALGORITHM == 'Q_LEARNING':\n",
    "            update_q_table(acting_q_table, old_state, action, reward, new_state, new_possible_actions)\n",
    "        \n",
    "        # elif CURRENT_ALGORITHM == 'SARSA':\n",
    "        #     # For SARSA, you would choose the *next* action right here\n",
    "        #     next_action = choose_action(acting_q_table, new_state, new_possible_actions, current_policy)\n",
    "        #     update_sarsa_table(acting_q_table, old_state, action, reward, new_state, next_action)\n",
    "        #     # ... and you'd have to save 'next_action' to use it in the next loop\n",
    "            \n",
    "        # --- 6. Stats & Terminal State Check ---\n",
    "        if agent_turn == 0:\n",
    "            total_reward_f += reward\n",
    "        else:\n",
    "            total_reward_m += reward\n",
    "            \n",
    "        current_run_steps += 1\n",
    "            \n",
    "        if is_terminal_state():\n",
    "            print(f\"--- Step {step}: Terminal state {terminal_states_reached + 1} reached in {current_run_steps} steps! ---\")\n",
    "            terminal_states_reached += 1\n",
    "            steps_per_run.append(current_run_steps)\n",
    "            current_run_steps = 0\n",
    "            \n",
    "            # \"if a terminal state is reached, restart... but do not reset the Q-table\" [cite: 211-212]\n",
    "            reset_world_and_agents() \n",
    "\n",
    "    print(\"\\n--- Simulation Finished ---\")\n",
    "    print(f\"Total Steps: {TOTAL_STEPS}\")\n",
    "    print(f\"Total Reward (Agent F): {total_reward_f}\")\n",
    "    print(f\"Total Reward (Agent M): {total_reward_m}\")\n",
    "    print(f\"Total Terminal States Reached: {terminal_states_reached}\")\n",
    "    if steps_per_run:\n",
    "        print(f\"Avg steps per run: {sum(steps_per_run) / len(steps_per_run):.2f}\")\n",
    "    print(f\"Total Q-Table size (F): {len(q_table_f)} states\")\n",
    "    print(f\"Total Q-Table size (M): {len(q_table_m)} states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b688d31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World and agents reset.\n",
      "Starting simulation: 8000 steps.\n",
      "Initial policy: PRANDOM\n",
      "--- Step 322: Terminal state 1 reached in 323 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 500: Switching policy to PEXPLOIT ---\n",
      "--- Step 521: Terminal state 2 reached in 199 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 731: Terminal state 3 reached in 210 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 950: Terminal state 4 reached in 219 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 1081: Terminal state 5 reached in 131 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 1310: Terminal state 6 reached in 229 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 1503: Terminal state 7 reached in 193 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 1679: Terminal state 8 reached in 176 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 1912: Terminal state 9 reached in 233 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 2078: Terminal state 10 reached in 166 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 2207: Terminal state 11 reached in 129 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 2301: Terminal state 12 reached in 94 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 2491: Terminal state 13 reached in 190 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 2583: Terminal state 14 reached in 92 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 2772: Terminal state 15 reached in 189 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 2836: Terminal state 16 reached in 64 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 2928: Terminal state 17 reached in 92 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 3012: Terminal state 18 reached in 84 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 3080: Terminal state 19 reached in 68 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 3210: Terminal state 20 reached in 130 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 3363: Terminal state 21 reached in 153 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 3436: Terminal state 22 reached in 73 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 3502: Terminal state 23 reached in 66 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 3571: Terminal state 24 reached in 69 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 3686: Terminal state 25 reached in 115 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 3800: Terminal state 26 reached in 114 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 3886: Terminal state 27 reached in 86 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 3929: Terminal state 28 reached in 43 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4042: Terminal state 29 reached in 113 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4117: Terminal state 30 reached in 75 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4197: Terminal state 31 reached in 80 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4256: Terminal state 32 reached in 59 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4320: Terminal state 33 reached in 64 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4429: Terminal state 34 reached in 109 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4521: Terminal state 35 reached in 92 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4591: Terminal state 36 reached in 70 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4693: Terminal state 37 reached in 102 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4782: Terminal state 38 reached in 89 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4847: Terminal state 39 reached in 65 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 4926: Terminal state 40 reached in 79 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5010: Terminal state 41 reached in 84 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5111: Terminal state 42 reached in 101 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5163: Terminal state 43 reached in 52 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5270: Terminal state 44 reached in 107 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5338: Terminal state 45 reached in 68 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5422: Terminal state 46 reached in 84 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5488: Terminal state 47 reached in 66 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5566: Terminal state 48 reached in 78 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5721: Terminal state 49 reached in 155 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5789: Terminal state 50 reached in 68 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5859: Terminal state 51 reached in 70 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5931: Terminal state 52 reached in 72 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 5975: Terminal state 53 reached in 44 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 6041: Terminal state 54 reached in 66 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 6086: Terminal state 55 reached in 45 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 6163: Terminal state 56 reached in 77 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 6220: Terminal state 57 reached in 57 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 6322: Terminal state 58 reached in 102 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 6443: Terminal state 59 reached in 121 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 6484: Terminal state 60 reached in 41 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 6586: Terminal state 61 reached in 102 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 6667: Terminal state 62 reached in 81 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 6748: Terminal state 63 reached in 81 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 6895: Terminal state 64 reached in 147 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7005: Terminal state 65 reached in 110 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7050: Terminal state 66 reached in 45 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7143: Terminal state 67 reached in 93 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7242: Terminal state 68 reached in 99 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7314: Terminal state 69 reached in 72 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7376: Terminal state 70 reached in 62 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7423: Terminal state 71 reached in 47 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7483: Terminal state 72 reached in 60 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7527: Terminal state 73 reached in 44 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7597: Terminal state 74 reached in 70 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7658: Terminal state 75 reached in 61 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7730: Terminal state 76 reached in 72 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7830: Terminal state 77 reached in 100 steps! ---\n",
      "World and agents reset.\n",
      "--- Step 7887: Terminal state 78 reached in 57 steps! ---\n",
      "World and agents reset.\n",
      "\n",
      "--- Simulation Finished ---\n",
      "Total Steps: 8000\n",
      "Total Reward (Agent F): 7060\n",
      "Total Reward (Agent M): 7046\n",
      "Total Terminal States Reached: 78\n",
      "Avg steps per run: 101.13\n",
      "Total Q-Table size (F): 1128 states\n",
      "Total Q-Table size (M): 1112 states\n",
      "\n",
      "--- Q-Table (Agent F) Sample ---\n",
      "State: (0, 2, False, 4, 2)\n",
      "  Actions: {'North': -0.3, 'South': -0.51, 'East': -0.3, 'West': 0.0, 'Pickup': 0.0, 'Dropoff': 0.0}\n",
      "State: (1, 2, False, 4, 2)\n",
      "  Actions: {'North': 0.0, 'South': 0.0, 'East': 0.0, 'West': 0.0, 'Pickup': 4.14715868625, 'Dropoff': 0.0}\n",
      "State: (1, 2, False, 4, 3)\n",
      "  Actions: {'North': -0.1528275, 'South': -0.3, 'East': 0.925136465181758, 'West': -0.25725, 'Pickup': 16.534050084190888, 'Dropoff': 0.0}\n",
      "State: (1, 2, True, 4, 3)\n",
      "  Actions: {'North': -0.8584499999999999, 'South': -0.8049, 'East': 7.068555560027799, 'West': -0.6475024642592687, 'Pickup': 0.0, 'Dropoff': 0.0}\n",
      "State: (1, 2, True, 4, 2)\n",
      "  Actions: {'North': -0.51, 'South': -0.657, 'East': 2.43093179175, 'West': -0.3, 'Pickup': 0.0, 'Dropoff': 0.0}\n",
      "State: (1, 3, True, 4, 2)\n",
      "  Actions: {'North': 0.0, 'South': 0.0, 'East': 0.0, 'West': 0.0, 'Pickup': 0.0, 'Dropoff': 0.0}\n",
      "State: (1, 3, True, 4, 3)\n",
      "  Actions: {'North': 1.5422306286484502, 'South': -0.51, 'East': -0.3, 'West': -0.51, 'Pickup': 0.0, 'Dropoff': 0.0}\n",
      "State: (1, 3, True, 4, 1)\n",
      "  Actions: {'North': -0.1528275, 'South': -0.3, 'East': -0.3, 'West': -0.3, 'Pickup': 0.0, 'Dropoff': 0.0}\n",
      "State: (1, 4, True, 4, 1)\n",
      "  Actions: {'North': 0.0, 'South': 0.0, 'East': 0.0, 'West': 0.0, 'Pickup': 0.0, 'Dropoff': 0.0}\n",
      "State: (1, 4, True, 4, 0)\n",
      "  Actions: {'North': 0.0, 'South': 0.0, 'East': -0.51, 'West': 0.0, 'Pickup': 0.0, 'Dropoff': 0.0}\n",
      "State: (2, 4, True, 4, 0)\n",
      "  Actions: {'North': 0.0, 'South': 0.0, 'East': 0.0, 'West': 0.0, 'Pickup': 0.0, 'Dropoff': 0.0}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_simulation()\n",
    "\n",
    "# You can add visualization code here, e.g.:\n",
    "print(\"\\n--- Q-Table (Agent F) Sample ---\")\n",
    "for i, (state, actions) in enumerate(q_table_f.items()):\n",
    "    if i > 10: break\n",
    "    print(f\"State: {state}\")\n",
    "    print(f\"  Actions: {actions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ig-reel-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
